---
title: "T13_Correlacion"
output: 
  html_document:
    toc: yes
    toc_float: yes
    css: custom.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)

```


```{r, eval=TRUE, echo=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}

#`r colorize("some words in red", "red")`


```

Fecha de la ultima revisión
```{r echo=FALSE}

Sys.Date()
```
***

## Install packages

```{r packages, echo=FALSE}
#Initiate packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(Hmisc, pastecs)
library(Hmisc)
library(pastecs)
library(ggplot2)

```


La correlaciones son métodos para evaluar la relación entre dos o más variables. 

Ejemplos

1. La cantidad de colesterol, LDL (lipoproteínas de baja densidad) y HDL (lipoproteínas de alta densidad).
2. La concentración de vitamina C y la absorción de calcio.
3. La consumo de alcohol y la concentración de alcohol en la sangre.


Los analísis se hacen comparando las variables continuas en pares. La covarianza es la medida de variabilidad conjunta entre dos variables. 




Primero recordamos la formula para calcular la varianza de una variable

$$s^{ 2 }=\frac { \sum _{ i=1 }^{ n }{ (x_{ i }-\bar { x } ) } ^{ 2 } }{ n-1 }$$


Ahora la vamos a descomponer 

$$s^{ 2 }=\frac { \sum _{ i=1 }^{ n }{ (x_{ i }-\bar{ x } ) } (x_{ i }-\bar { x } ) }{ n-1 }$$
***
# Correlación de Pearson

Esta la más común y conocida como el Coeficiente de correlación de Pearson. Nota ahora que tenemos dos variables **x** y **y**.   


Podemos ver cual es la relación entre la edad de los estudiantes y cuantas veces fueron a Disney en Orlando, Florida. El objetivo es evaluar el covarianza entre una variable y otra.   


|Personas       	| Maria 	| Juan 	| José  	| Carla  	| Luis  	|
|----------------	|---	|---	|----	|----	|----	|
| Edad 	  (x)        | 8 	| 9 	| 10 	| 13 	| 15 	|
| Veces a Disney (y) | 5 	| 4 	| 4  	| 6  	| 8  	|



La siguiente formula muestra el calculo para el coeficiente de Pearson $\rho$ que se dice **rho** que es el parámetro (el universo). Para significar el coeficiente de una muestra se usa la **r**.   

$$r=\frac { cov(x,y) }{ { s }_{ x }{ s }_{ y } } =\frac { \sum { ({ x }_{ i }-\bar { x } _{ i })({ y }_{ i }-\bar { y } _{ i }) }  }{ n-1({ s }_{ x }{ s }_{ y }) }$$
Una otra manera de ver como se calcula el coeficiente es usar la siguiente formula.  

$$r=\frac{\sum_{ }^{ }xy-\left(\frac{\sum_{ }^{ }x\sum_{ }^{ }y}{n}\right)}{\sqrt{\left(\sum_{ }^{ }x^2-\frac{\left(\sum_{ }^{ }x\right)^2}{n}\right)\left(\sum_{ }^{ }y^2-\frac{\left(\sum_{ }^{ }y\right)^2}{n}\right)}}$$



Creamos un conjunto de datos en R para demostrar como se calcula a mano.  


```{r simple example}
Edad<-c(8,9,10,13,15)
Disney<-c(5,4,4,6,8)
dfDisney<-data.frame(Edad, Disney) # unir las listas en un data frame
dfDisney$Edad2=dfDisney$Edad^2 # cuadrar las x
dfDisney$Disney2=dfDisney$Disney^2 # cuadrar las y
dfDisney$Edad_por_Disney=dfDisney$Edad*dfDisney$Disney # productos cruzados x*y
dfDisney
```
Lo que necesitamos ahora es calcular cada una de las variables


```{r, fig.align='center' }

library(knitr)
library(kableExtra)
df <- data.frame(Formulas = c("$$\\sum x $$","$$\\sum y$$",
                              "$$\\sum x^2$$", "$$\\sum y^2$$", 
                              "$$\\sum xy$$"),
                 Sumas = c(55, 27, 639,157, 314))

knitr::kable(df,  align="lr",  escape=FALSE)

```

Ahora sustituir los valores en la ecuación para calcular el coeficiente de Pearson.

$$r=\frac{314-\left(\frac{55\cdot27}{5}\right)}{\sqrt{\left(639-\frac{\left(55\right)^2}{5}\right)\left(157-\frac{\left(27\right)^2}{5}\right)}}=0.871$$
La función para hacer todos estos calculo es **cor**. Con una correlación de 0.87, esto significa que hay buena correlación positiva entre la edad y la cantidad de veces que han ido a Disney.    

```{r}
cor(Edad, Disney)
```


***

### Un gráfico de Correlación sencilla. 

Parece haber un patrón de cuando aumenta una variable aumenta la otra. 

```{r graph correlations,  out.width = '60%',fig.align='center'}
scatter<-ggplot(dfDisney, aes(x=Edad, y=Disney))
scatter + geom_point(colour="purple")+
  geom_hline(aes(yintercept=mean(Disney)))+ # las lineas representa el promedio de la variable en *y* los valores
  geom_vline(aes(xintercept=mean(Edad))) # las lineas representa el promedio de la variable en *x* los valores


```
***
# El rango del coeficiente Pearson

El coeficiente de correlación varía de −1 a 1. El valor de 1 implica que la relación entre X e Y es perfecta y positiva, en otra palabra la **x** predice exactamente el valor de **y**.  Un valor de -1 implica que los datos predicen una relación negativa perfecta entre la **x** y **y**. Cuando el valor de 0 o cerca esto implica que no existe una correlación lineal entre las variables.

***
## Los Supuestos de la prueba de Pearson


1. Uno de los supuestos de la prueba de análisis de Pearson, es que las dos variables tenga una distribución normal,  **Nornal bivariada**.
2. Las variables tienen que tener una correlación lineal (por ejemplo: no puede ser cuadrática o logarítmica).

3. Ausencia de valores átipicos (o por lo menos sesgan el resultado)

Para visualizar una distribución vea lo siguiente. Nota que las dos variables tienen una distribución normal. 

***
## Gráfico Bivariado Normal


https://deanattali.com/2015/03/29/ggExtra-r-package/

Para visualizar que quiere decir "bivariado normal"  Vamos a crear dos conjuntos de datos cada una con una distribución normal, pero con promedio diferentes y desviación estandar.  
```{r}
library(tidyverse)
a=rnorm(8000, 0, 1)
b=rnorm(8000, 1, 1.1)

df=data.frame(a,b)

out <- stack(df)
 
ggplot(out, aes(values, colour=ind, fill=ind))+
  geom_histogram(bins=50)+
  facet_wrap(~ind)


```
Ahora visualizamos las dos variables en el mismo gráfico. y se añade en el margen.  

```{r}

#install.packages("ggExtra", dependencies = TRUE)
library(ggExtra)

p <- ggplot(df, aes(a, b)) + geom_point() + 
  theme_classic()
# add marginal histograms
ggExtra::ggMarginal(p, type = "histogram")

```

***

# Correlación no-paramétricas

## Correlación de Kendall


En estadística, el coeficiente de correlación de Kendall se calcula usando los rangos y no los valores originales, es conocido como el coeficiente tau de Kendall (letra griega τ), es una estadística para medir la asociación ordinal ( en otra palabra el orden de los valores) entre dos variables. La prueba de Kendall es una prueba de hipótesis no paramétrica.  Entonces no asumo que los datos tenga una distribución normal.  El nombre de la prueba proviene de Maurice Kendall quien desarrolló el método en 1938.  

La diferencias entre este prueba es que los valores se poner en orden del más pequeño al más grande (en cada variable) y se usa el número del ordenamiento para hacer la prueba.   

La correlación de Kendall entre dos variables será alta cuando las observaciones tengan un rango similar (o idéntico para una correlación de +1) (es decir, la posición relativa de las observaciones dentro de la variable: primero, segundo, tercero, etc.) entre las dos variables, y bajo cuando las observaciones tienen un rango diferente (o completamente diferente para una correlación de -1) entre las dos variables.

Aquí un pequeño conjunto de datos como ejemplo de ordenar los datos.  Nota que después de ordenarlos datos son estos que se usan para al calculo.  La edad de la persona, el número de veces que fueron a Disney y el orden de cada variable.  


| Edad | Disney | Orden Edad | Orden Disney |
|------:|--------:|------------:|--------------:|
| 8    | 8      | 3          | 4            |
| 2    | 2      | 1          | 2            |
| 10   | 12     | 4          | 5            |
| 54   | 0      | 5          | 1            |
| 3    | 6      | 2          | 3            |

***

### Los supuestos de Kendall

 1. Los pares de observaciones son independientes.
 2. Se deben medir las dos variables en una escala ordinal, de intervalo o de razón.
 3. Se supone que existe una relación monotónica entre las dos variables.
 
***

## Correlación de Spearman


El coeficiente de correlación de rango de Spearman o el $\rho$ rho de Spearman, desarrollado por Charles Spearman en 1904 era un psicologo.  Es una medida no paramétrica de correlación de rango (como la correlación de Kendall). Igual como,la de Kendall usa los rangos para hacer el estadístico, pero usan formulas distinctas.  

La correlación de Spearman entre dos variables será alta cuando las observaciones tengan un rango similar (o idéntico para una correlación de +1) (es decir, la posición relativa de las observaciones dentro de la variable: primero, segundo, tercero, etc.) entre las dos variables, y bajo cuando las observaciones tienen un rango diferente (o completamente diferente para una correlación de -1) entre las dos variables.

***
### Los supuestos de Spearman

 1. Los pares de observaciones son independientes.
 2. Se deben medir las dos variables en una escala ordinal, de intervalo o de razón.
 3. Se supone que existe una relación monotónica entre las dos variables.
 
*** 
## Comparación entre Kendall y Spearman

La prueba de Kendall tiende a ser más robusta que la Spearman. Lo que esto significa es que cuando el tamaño de muestra es pequeña o hay valores átipicos la prueba de Kendall es preferible.  

***

# Los codigos en R para correlación

## Como lidiar con "NA" alternativas
Se usa diferentes 

use = "everything",   Si Hay 1 "NA" en la los datos, elimina la variable completa
use = "complete.obs", Si hay 1 "NA", Elimina el individuo que tiene "NA" para todas los cálculos
use = "pairwise.complete.obs", Si hay un "NA", Elimina el individuo solamente si el par evaluado tiene un "NA". 



```{r missing values}
#-----Como lidiar con los "NA"

adverts<-c(15,14,14,6,2)
comprasNA<-c(8,9,10,NA,15)
age<-c(5, 12, 16, 9, 14)
advertNA<-data.frame(adverts, comprasNA, age) # el data frame

# "everything", "complete.obs", "pairwise.complete.obs"

head(advertNA)
cor(advertNA, use = "everything",  method = "pearson")
cor(advertNA, use = "complete.obs",  method = "pearson")
cor(advertNA, use = "pairwise.complete.obs",  method = "pearson") # Comúnmente usado.  

```
*** 
# La pruebas con los métodos de Kendall y Spearman

```{r, kendall and Spearman}
cor(advertNA, use = "pairwise.complete.obs",  method = "kendall")   
cor(advertNA, use = "pairwise.complete.obs",  method = "spearman") # n= 30 or more


```

## Ejemplo de Anxiedad y correlaciones

Corelación entre anxiedad, la nota de examen y el tiempo que estudio el estudiante. 


Anxiety data
```{r example with anxiety}

library(readr)
Exam_Anxiety <- read_csv("~/Google Drive/Biometry/Biometria 2015/Biometria_2015_RStudio/Data files/Exam_Anxiety.csv")
examData=Exam_Anxiety
head(examData)

```


Correlation anaylisis between anxiety, Exam and time for study (revise)


```{r correlation anxiety}
cor(examData$Exam, examData$Anxiety, use = "complete.obs",
    method = 'pearson')



examData2 <- examData[, c("Exam", "Anxiety", "Revise")]
head(examData2)
cor(examData2)  # range from -1 to + 1
cor(examData[, c("Exam", "Anxiety", "Revise")])

cor(examData2)^2 * 100 # Ranges from 0 to 100  The proportion of the variance explained

#Coeficiente de determinación
```

Using the corr.test function


```{r}
ggplot(examData2,aes(Revise, Anxiety))+
  geom_point()
```

```{r}
ggplot(examData,aes(Revise, Exam, colour=Gender))+
  geom_point()
```



```{r corr.test function}
cor.test(examData$Anxiety, examData$Exam)
cor.test(examData$Revise, examData$Exam)
cor.test(examData$Anxiety, examData$Revise)



```

















The biggest liar data


```{r liar data}
library(ggplot2)
library(readr)
The_Biggest_Liar <- read_csv("~/Google Drive/Biometry/Biometria 2015/Biometria_2015_RStudio/Data files/The Biggest Liar.csv")

liarData=The_Biggest_Liar 
liarData
length(liarData$Creativity)
summary(liarData)
```

Como cambiar el nombre de la variables

```{r}
names(liarData)
names(liarData)[1]=paste("Creatividad")  # se cambia el nombre de la columna
names(liarData)
names(liarData)[2]=paste("Posición")
names(liarData)[3]=paste("Principiante")
head(liarData)
```

Evaluar si las variables tienen distribución normal

```{r}

p <- ggplot(liarData, aes(Creatividad, Posición)) + 
  geom_point() + 
  theme_classic()
p
# add marginal histograms
ggExtra::ggMarginal(p, type = "histogram")
```


```{r}
ggplot(liarData, aes(Posición))+ 
  geom_histogram(fill="blue", colour="white")

ggplot(liarData, aes(Creatividad))+
  geom_histogram()

```


Spearman Correlations


```{r, out.width="500px"}

library(knitr)
library(png)

img4_path<-"Graficos/Stanton.png"
include_graphics(img4_path)
```

```{r Spearman}
#-------Spearman correlation-----Use when data are not normal, 
# such as ordinal data

cor(liarData$Posición, liarData$Creatividad, method = "spearman")



cor.test(liarData$Posición, liarData$Creatividad, method = "spearman")
cor.test(liarData$Posición, liarData$Creatividad, alternative = "less", method = "kendall")


ggplot(liarData,aes(Posición, Creatividad))+
  geom_point()
```

```{r}
ggplot(liarData, aes(Posición, Creatividad))+
  geom_point()
```



Another alternative

```{r rcorr}
library(Hmisc)  # for the function rcorr
liarMatrix<-as.matrix(liarData[, c("Posición", "Creatividad", "Principiante")])

rcorr(liarMatrix, type=c("spearman")) # si no pone nada asume distribución normal y usa Pearson
#--------Kendall's Tau----------Use with small sample size with tied ranks
# use "less" or "greater", depending if you 
# predict that the correlation will be less or greater than zero

#cor(liarData$Posición, liarData$Creatividad, method = "kendall")
#cor.test(liarData$Posición, liarData$Creatividad, alternative = "less", method = "kendall")

```

```{r hist anxiety}
ggplot(examData, aes(Exam))+
         geom_histogram()

ggplot(examData, aes(Anxiety))+
         geom_histogram()
```


```{r kendall exam anxiety}
cor.test(examData$Anxiety, examData$Exam, method = "kendall")

ggplot(examData, aes(Anxiety, Exam))+
  geom_point()
```




