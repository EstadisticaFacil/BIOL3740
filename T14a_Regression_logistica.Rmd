---
title: "T14a_Regression_logistica"
output: 
  html_document:
    toc: yes
    toc_float: yes
    css: custom.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)

```


```{r, eval=TRUE, echo=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}

#`r colorize("some words in red", "red")`


```

Fecha de la ultima revisión
```{r echo=FALSE}

Sys.Date()
```

# Regresión Logistica

Esta regresión es utilizada cuando nuestra variable dependiente tiene solamente dos alternativas (representadas de forma numérica, **0 y 1**) y la variable independiente es una variable continua.

## *Brassavola cucullata*
Los datos fueron recolectado de dos pequeñas islas del Caribe, San Eustaquio y Saba.  

*Brassavola cucullata* pertenece a la subtribu Laeliinae y es una especie epífita y rupícola que puede formar grandes racimos de brotes. Cada brote está compuesto por un solo tallo de 3.5-12.5 cm de largo y 1-3.5 mm de diámetro y tiene una sola hoja semi-tereta de 16-35 cm de largo y solo un poco más gruesa que el tallo. Las inflorescencias terminales miden 3-30 mm de largo y suelen tener una sola flor. Las flores son en gran parte blancas con las partes delgadas del perianto que a menudo se vuelven de color amarillo pálido hacia sus ápices. El labelo es ovado-acuminado y fimbriado alrededor de la columna. El cunículo se extiende hacia el ovario inferior y no tiene néctar. Las flores engañosas tienen una fragancia nocturna dulce y espesa, que puede perdurar hasta el día. La producción de frutos depende de los polinizadores. Las cápsulas tardan varios meses en desarrollarse y son pediceladas y picudas (restos de la columna); el cuerpo de la cápsula mide entre 2 y 5 cm de largo y produce muchos miles de semillas polvorientas (Ackerman y Collaborators 2014).

En cada una de las tres poblaciones, etiquetamos todas las plantas de *B. cucullata* que pudimos encontrar. Observamos si cada planta era epífita o epilítica, medimos la altura sobre el suelo, buscamos evidencia de herbivoría foliar, medimos la longitud de la hoja más larga y contamos el número de brotes de hojas, flores y frutos. Si las flores estaban presentes, registramos si habían sido visitadas con éxito o no mediante una inspección visual para la eliminación de polinarios o polinias en el estigma. Estos datos se obtuvieron una vez al año durante la época de floración. Nuestras observaciones en la población de Quill abarcaron 2009-2013, en Boven 2010-2013 y en Saba 2011-2014.


Puede encontrar en manuscrito en este enlace, que evalúa la biología de la orquídea y la posibilidad de extinción y como las cabras impacta su supervivencias.     

Pronto

```{r data}
library(readr)
Student_Brassavola <- read_csv("Data_files_csv/Student_Brassavola.csv")
completeBrass=na.omit(Student_Brassavola)  # remove NA
head(completeBrass)
```

Evaluar la tendencias centrales y la dispersión de las variables y cambio de nombre de las variables al español.


```{r}

Brass=
  completeBrass%>%
  rename(Isla=Island, Ano=Year, Número_planta=Pl_Num, Cant_hojas=Leaf_Num, Largo_Hoja_cm=LLL, Cant_capullo=Bud_Num, Cant_Flores=Fl_Num, Cant_Frutos=Fr_Num, BQS=BQS, Flor_si_no=Flowers_Y_N)

summary(Brass)
```

## Look at the data

You can also embed plots, for example:

```{r head, echo=FALSE}

library(ggplot2)

ggplot(Brass, aes(Flor_si_no))+
geom_histogram()
```




# Modelo Lineal Generalizado
En este modulo se hace una primera introducción a un otro tipo de herramienta para el análisis de datos, denominado "Modelo Lineal Generalizado". Lo interesante de esta acercamiento es que aun que uno tiene una variable de respuesta que no cumple con distribución normal hay múltiples opciones para los análisis.  En este modulo solamente se estará presentando el tipo de datos donde la variable de respuesta (Y) es binomial. 

Hay tres componentes en cualquier GLM:

Componente aleatorio: se refiere a la distribución de probabilidad de la variable de respuesta (Y); p.ej. distribución binomial (0, 1,: Si o NO: Vivo o Muerto) para Y en la regresión logística binaria. Nota que la variable Y puede tener muchas otras tipo de distribución incluyendo la distribución normal.  

Componente sistemático: especifica las variables explicativas (X1, X2, ... Xk) en el modelo, más específicamente su combinación lineal en la creación del llamado predictor lineal; por ejemplo, β0 + β1x1 + β2x2 como hemos visto en una regresión lineal, o como veremos en una regresión logística en esta lección.

Función de enlace, η o g (μ): especifica el enlace entre componentes aleatorios y sistemáticos. Dice cómo el valor esperado de la respuesta se relaciona con el predictor lineal de variables explicativas; por ejemplo, η = g (E (Yi)) = E (Yi) para la regresión lineal, o η = logit (π) para la regresión logística.



logit(π) = la probabilidad de un evento
Assumptions:

1.  The data Y1, Y2, ..., Yn are independently distributed, i.e., cases are independent.
2.  The dependent variable Yi does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,...)
3. GLM does NOT assume a linear relationship between the dependent variable and the independent variables, but it does assume linear relationship between the transformed response in terms of the link function and the explanatory variables; e.g., for binary logistic regression logit(π) = β0 + βX.
4. Independent (explanatory) variables can be even the power terms or some other nonlinear transformations of the original independent variables.
5. The homogeneity of variance does NOT need to be satisfied. In fact, it is not even possible in many cases given the model structure, and overdispersion (when the observed variance is larger than what the model assumes) maybe present.
6. Errors need to be independent but NOT normally distributed.
7. It uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations.
8. Goodness-of-fit measures rely on sufficiently large samples, where a heuristic rule is that not more than 20% of the expected cells counts are less than 5.

For a more detailed discussion refer to Agresti(2007), Ch. 3, Agresti (2013), Ch.4, and/or McCullagh & Nelder (1989).

Following are examples of GLM components for models that we are already familiar, such as linear regression, and for some of the models that we will cover in this class, such as logistic regression and log-linear models

## Simple Linear Model 
Simple Linear Regression models how mean expected value of a continuous response variable depends on a set of explanatory variables, where index i stands for each data point:

Yi=β0+βxi+ϵi

or

E(Yi)=β0+βxi 

1. Random component: Y is a response variable and has a normal distribution, and generally we assume errors, ei ~ N(0, σ2).
2.Systematic component: X is the explanatory variable (can be continuous or discrete) and is linear in the parameters β0  + βxi . Notice that with a multiple linear regression where we have more than one explanatory variable, e.g., (X1, X2, ... Xk), we would have a linear combination of these Xs in terms of regression parameters β's, but the explanatory variables themselves could be transformed, e.g., X2, or log(X). 
3. Link function: Identity Link, η = g(E(Yi)) = E(Yi) --- identity because we are modeling the mean directly; this is the simplest link function.

# Logistic Regression 
Binary Logistic Regression models how binary response variable Y depends on a set of k explanatory variables, X=(X1, X2, ... Xk). 

logit(π)=log(π1−π)=β0+βxi+…+β0+βxk′

which models the log odds of probability of "success" as a function of explanatory variables.

1. Random component: The distribution of Y is assumed to be Binomial(n,π), where π is a probability of "success". 
2. Systematic component: X's are explanatory variables (can be continuous, discrete, or both) and are linear in the parameters, e.g.,  β0 + βxi + ... + β0 + βxk. Again, transformation of the X's themselves are allowed like in linear regression; this holds for any GLM. 
3. Link function: Logit link:

η=logit(π)=log(π1−π)

More generally, the logit link models the log odds of the mean, and the mean here is π. Binary logistic regression models are also known as logit models when the predictors are all categorical.

# Generalized Linear Model = glm

```{r model}

BrassModel.1 <- glm(Flowers_Y_N ~ Leaf_Num,
                    data = Brass, family = binomial())

summary(BrassModel.1)
# generalized linear model; glm 

BrassModel.1LLL <- glm(Flowers_Y_N ~ LLL,
                    data = Brass, family = binomial())

summary(BrassModel.1LLL)

BrassModel.2 <- glm(Flowers_Y_N ~ LLL+Leaf_Num,
                    data = Brass, family = binomial())
summary(BrassModel.2)

summary(BrassModel.1)
summary(BrassModel.1LLL)
summary(BrassModel.2)

# AIC = Akaike Information Criterion



```

Si no le interesa el interecepto se añade un "-1" despues de la 
#Los valores de interes en nuestro caso son el intercepto y la pendiente (valor que se encuentra debajo del intercepto).

```{r model bby site surveyed}
BrassModel.1.1 <- glm(Flowers_Y_N ~ Leaf_Num+BQS+LLL-1,
                    data = Brass, family = binomial())
summary(BrassModel.1.1)
```

# Predicting the number of Fruits from the equation, using the results from the model

$$P(Y)\quad =\quad \frac { 1 }{ 1+{ e }^{ -(b+m*{ x }_{ i }) } } $$
$$P(Y)\quad =\quad \frac { 1 }{ 1+{ e }^{ -((intercepto)+pendiente*variable.predictora) } } $$
# se usa esta ecuacion para predecir un valor de 'Y' especifico para un valor de una variable 'X' de interes.

```{r predicting num fruits}

summary(BrassModel.1)

exp(1) #= e
e=exp(1)
e


P_10=1/(1+2.7182818284^-(-3.065 +0.0297*150))
P_10

P_10=1/(1+exp(1)^-(-3.065 +0.0297*100))
P_10
P_25=1/(1+exp(-(-3.065 +0.0297*25)))
P_25

P_50=1/(1+e^-(-3.065 +0.0297*50))
P_50


P_70=1/(1+e^-(-3.065 +0.0297*70))
P_70

P_150=1/(1+e^-(-3.065 +0.0297*150))
P_150
```


```{r ggplot binomial}
library(ggplot2)
 ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N))+
  geom_point()

```


```{r jitter}

ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N))+
  geom_jitter(height = 0.10)
```

#Create a function to calculate the logistic regression figure called "binomial_smooth"

```{r binomial model}

binomial_smooth <- function(...) {
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), ...)
}
```


# Graphic with binomial fit

```{r binomial fit}
ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N))+
  geom_point()+
  binomial_smooth()

model1=glm(Flowers_Y_N~Leaf_Num, data=completeBrass,
           family = binomial( ))
summary(model1)
```
#En este grafico se osberva el valor de la pendiente, el mismo aumenta cuando el numero de hojas se aproxima a 100 (por ende al acercarnos a este valor el valor de nuestra variable de respuesta es mayor aumenta o es mayor).
## add a figure for each site

```{r facet}
names(completeBrass)
ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N,colour=Island))+
  geom_point()+
  binomial_smooth()+
  facet_wrap(~Island)

```


## overlap figure

```{r overlap by site}


ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N, colour=BQS))+
  geom_point()+
  binomial_smooth()

```


# Evaluate using the Length of the longest leaf

```{r LLL}

ggplot(completeBrass, aes(LLL,Flowers_Y_N))+
  geom_jitter(height = 0.25)

```


### Remove the outlier (probably a mistake in data entry)

#### Nota que se hace un subgrupo (subset) de los datos, usando la función subset(el data frame, la condición)


```{r LLL90}

ggplot(subset(completeBrass,LLL<90), aes(LLL,Flowers_Y_N, colour=BQS))+
  geom_jitter(height = 0.25)+
  binomial_smooth()
```


Choosing from different models, which is the best model.  

The R^2 versus the Akaike Information criterion (AIC)

https://www.theanalysisfactor.com/r-glm-model-fit/




Deviance

We see the word Deviance twice over in the model output. Deviance is a measure of goodness of fit of a generalized linear model. Or rather, it’s a measure of badness of fit–higher numbers indicate worse fit.

R reports two forms of deviance – the null deviance and the residual deviance. The null deviance shows how well the response variable is predicted by a model that includes only the intercept (grand mean).

For our example, we have a value of 910.24  on 1513 degrees of freedom. Including the independent variables (weight and displacement) decreased the deviance to 777.02 points on 1512 degrees of freedom, a significant reduction in deviance.

The Residual Deviance has reduced by 910.24-777.02=133.22 with a loss of one degrees of freedom.

```{r}

BrassModel.1 <- glm(Flowers_Y_N ~ Leaf_Num,
                    data = Brass, family = binomial())
summary(BrassModel.1)
```




Akaike Information Criteron

Information Criteria

The Akaike Information Criterion (AIC) provides a method for assessing the quality of your model through comparison of related models.  It’s based on the Deviance, but penalizes you for making the model more complicated.  Much like adjusted R-squared, it’s intent is to prevent you from including irrelevant predictors.

However, unlike adjusted R-squared, the number itself is not meaningful. If you have more than one similar candidate models (where all of the variables of the simpler model occur in the more complex models), then you should select the model that has the smallest AIC.

So it’s useful for comparing models, but isn’t interpretable on its own. YOU NEED TO COMPARE MODELS. 






```{r}


# Complex models

Brass$LLLxLN=Brass$LLL*Brass$Leaf_Num  # add a new variable the multiplication of LLL* Leaf_Num
#head(Brass)

BrassModel.1 <- glm(Flowers_Y_N ~ LLL,
                    data = Brass, family = binomial())


BrassModel.2 <- glm(Flowers_Y_N ~ Leaf_Num,
                    data = Brass, family = binomial())

BrassModel.3 <- glm(Flowers_Y_N ~ LLLxLN,
                    data = Brass, family = binomial())

summary(BrassModel.1)
summary(BrassModel.2)
summary(BrassModel.3)