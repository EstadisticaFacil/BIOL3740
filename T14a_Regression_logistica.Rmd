---
title: "T14a_Regression_logistica"
output: 
  html_document:
    toc: yes
    toc_float: yes
    css: custom.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)

```


```{r, eval=TRUE, echo=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}

#`r colorize("some words in red", "red")`


```

Fecha de la ultima revisión
```{r echo=FALSE}

Sys.Date()
```

# Regresión Logistica

Esta regresión es utilizada cuando nuestra variable dependiente tiene solamente dos alternativas (representadas de forma numérica, **0 y 1**) y la variable independiente es una variable continua.


```{r data}
library(readr)
Student_Brassavola <- read_csv("Data_files_csv/Student_Brassavola.csv")
head(Student_Brassavola)
Brass=Student_Brassavola
completeBrass=na.omit(Brass)  # remove NA
head(completeBrass)
summary(completeBrass)
```

## Look at the data

You can also embed plots, for example:

```{r head, echo=FALSE}

library(ggplot2)

ggplot(completeBrass, aes(Flowers_Y_N))+
geom_histogram()
```


# Generalized Linear Model:  

https://onlinecourses.science.psu.edu/stat504/node/216/


Copied from the above website....

6.1 - Introduction to Generalized Linear Models

Thus far our focus has been on describing interactions or associations between two or three categorical variables mostly via single summary statistics and with significance testing. Models can handle more complicated situations and analyze the simultaneous effects of multiple variables, including mixtures of categorical and continuous variables. For example, the Breslow-Day statistics only works for 2 × 2 × K tables, while log-linear models will allow us to test of homogeneous associations in I × J × K and higher-dimensional tables. We will focus on a special class of models known as the generalized linear models (GLIMs or GLMs in Agresti).

The structural form of the model describes the patterns of interactions and associations. The model parameters provide measures of strength of associations. In models, the focus is on estimating the model parameters. The basic inference tools (e.g., point estimation, hypothesis testing, and confidence intervals) will be applied to these parameters. When discussing models, we will keep in mind:

Objective
Model structure (e.g. variables, formula, equation)
Model assumptions
Parameter estimates and interpretation
Model fit (e.g. goodness-of-fit tests and statistics)
Model selection:
For example, recall a simple linear regression model

Objective: model the expected value of a continuous variable, Y, as a linear function of the continuous predictor, X, E(Yi) = β0 + β1xi
Model structure: Yi = β0 + β1xi + ei
Model assumptions: Y is is normally distributed, errors are normally distributed, $e_{i} ∼ N(0, σ2)$, and independent, and X is fixed, and constant variance σ2.
Parameter estimates and interpretation: β̂0 is estimate of β0 or the intercept, and β̂1 is estimate of the slope, etc... 


Do you recall, what is the interpretation of the intercept and the slope?


Model fit: R 2, residual analysis, F-statistic
Model selection: From a plethora of possible predictors, which variables to include?

# Generalized Linear Models: Modelo Lineal Generalizado
## GLM

There are three components to any GLM:

Random Component – refers to the probability distribution of the response variable (Y); e.g. normal distribution for Y in the linear regression, or binomial distribution (0, 1, : Si o NO: Vivo o Muerto) for Y in the binary logistic regression.  Also called a noise model or error model.  How is random error added to the prediction that comes out of the link function?
Systematic Component - specifies the explanatory variables (X1, X2, ... Xk) in the model, more specifically their linear combination in creating the so called linear predictor; e.g., β0 + β1x1 + β2x2 as we have seen in a linear regression, or as we will see in a logistic regression in this lesson.
Link Function, η or g(μ) - specifies the link between random and systematic components. It says how the expected value of the response relates to the linear predictor of explanatory variables; e.g., η = g(E(Yi)) = E(Yi) for linear regression, or  η = logit(π) for logistic regression.

logit(π) = la probabilidad de un evento
Assumptions:

1.  The data Y1, Y2, ..., Yn are independently distributed, i.e., cases are independent.
2.  The dependent variable Yi does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,...)
3. GLM does NOT assume a linear relationship between the dependent variable and the independent variables, but it does assume linear relationship between the transformed response in terms of the link function and the explanatory variables; e.g., for binary logistic regression logit(π) = β0 + βX.
4. Independent (explanatory) variables can be even the power terms or some other nonlinear transformations of the original independent variables.
5. The homogeneity of variance does NOT need to be satisfied. In fact, it is not even possible in many cases given the model structure, and overdispersion (when the observed variance is larger than what the model assumes) maybe present.
6. Errors need to be independent but NOT normally distributed.
7. It uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations.
8. Goodness-of-fit measures rely on sufficiently large samples, where a heuristic rule is that not more than 20% of the expected cells counts are less than 5.

For a more detailed discussion refer to Agresti(2007), Ch. 3, Agresti (2013), Ch.4, and/or McCullagh & Nelder (1989).

Following are examples of GLM components for models that we are already familiar, such as linear regression, and for some of the models that we will cover in this class, such as logistic regression and log-linear models

## Simple Linear Model 
Simple Linear Regression models how mean expected value of a continuous response variable depends on a set of explanatory variables, where index i stands for each data point:

Yi=β0+βxi+ϵi

or

E(Yi)=β0+βxi 

1. Random component: Y is a response variable and has a normal distribution, and generally we assume errors, ei ~ N(0, σ2).
2.Systematic component: X is the explanatory variable (can be continuous or discrete) and is linear in the parameters β0  + βxi . Notice that with a multiple linear regression where we have more than one explanatory variable, e.g., (X1, X2, ... Xk), we would have a linear combination of these Xs in terms of regression parameters β's, but the explanatory variables themselves could be transformed, e.g., X2, or log(X). 
3. Link function: Identity Link, η = g(E(Yi)) = E(Yi) --- identity because we are modeling the mean directly; this is the simplest link function.

# Logistic Regression 
Binary Logistic Regression models how binary response variable Y depends on a set of k explanatory variables, X=(X1, X2, ... Xk). 

logit(π)=log(π1−π)=β0+βxi+…+β0+βxk′

which models the log odds of probability of "success" as a function of explanatory variables.

1. Random component: The distribution of Y is assumed to be Binomial(n,π), where π is a probability of "success". 
2. Systematic component: X's are explanatory variables (can be continuous, discrete, or both) and are linear in the parameters, e.g.,  β0 + βxi + ... + β0 + βxk. Again, transformation of the X's themselves are allowed like in linear regression; this holds for any GLM. 
3. Link function: Logit link:

η=logit(π)=log(π1−π)

More generally, the logit link models the log odds of the mean, and the mean here is π. Binary logistic regression models are also known as logit models when the predictors are all categorical.

# Generalized Linear Model = glm

```{r model}

BrassModel.1 <- glm(Flowers_Y_N ~ Leaf_Num,
                    data = Brass, family = binomial())

summary(BrassModel.1)
# generalized linear model; glm 

BrassModel.1LLL <- glm(Flowers_Y_N ~ LLL,
                    data = Brass, family = binomial())

summary(BrassModel.1LLL)

BrassModel.2 <- glm(Flowers_Y_N ~ LLL+Leaf_Num,
                    data = Brass, family = binomial())
summary(BrassModel.2)

summary(BrassModel.1)
summary(BrassModel.1LLL)
summary(BrassModel.2)

# AIC = Akaike Information Criterion



```

Si no le interesa el interecepto se añade un "-1" despues de la 
#Los valores de interes en nuestro caso son el intercepto y la pendiente (valor que se encuentra debajo del intercepto).

```{r model bby site surveyed}
BrassModel.1.1 <- glm(Flowers_Y_N ~ Leaf_Num+BQS+LLL-1,
                    data = Brass, family = binomial())
summary(BrassModel.1.1)
```

# Predicting the number of Fruits from the equation, using the results from the model

$$P(Y)\quad =\quad \frac { 1 }{ 1+{ e }^{ -(b+m*{ x }_{ i }) } } $$
$$P(Y)\quad =\quad \frac { 1 }{ 1+{ e }^{ -((intercepto)+pendiente*variable.predictora) } } $$
# se usa esta ecuacion para predecir un valor de 'Y' especifico para un valor de una variable 'X' de interes.

```{r predicting num fruits}

summary(BrassModel.1)

exp(1) #= e
e=exp(1)
e


P_10=1/(1+2.7182818284^-(-3.065 +0.0297*150))
P_10

P_10=1/(1+exp(1)^-(-3.065 +0.0297*100))
P_10
P_25=1/(1+exp(-(-3.065 +0.0297*25)))
P_25

P_50=1/(1+e^-(-3.065 +0.0297*50))
P_50


P_70=1/(1+e^-(-3.065 +0.0297*70))
P_70

P_150=1/(1+e^-(-3.065 +0.0297*150))
P_150
```


```{r ggplot binomial}
library(ggplot2)
 ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N))+
  geom_point()

```


```{r jitter}

ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N))+
  geom_jitter(height = 0.10)
```

#Create a function to calculate the logistic regression figure called "binomial_smooth"

```{r binomial model}

binomial_smooth <- function(...) {
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), ...)
}
```


# Graphic with binomial fit

```{r binomial fit}
ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N))+
  geom_point()+
  binomial_smooth()

model1=glm(Flowers_Y_N~Leaf_Num, data=completeBrass,
           family = binomial( ))
summary(model1)
```
#En este grafico se osberva el valor de la pendiente, el mismo aumenta cuando el numero de hojas se aproxima a 100 (por ende al acercarnos a este valor el valor de nuestra variable de respuesta es mayor aumenta o es mayor).
## add a figure for each site

```{r facet}
names(completeBrass)
ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N,colour=Island))+
  geom_point()+
  binomial_smooth()+
  facet_wrap(~Island)

```


## overlap figure

```{r overlap by site}


ggplot(completeBrass, aes(Leaf_Num,Flowers_Y_N, colour=BQS))+
  geom_point()+
  binomial_smooth()

```


# Evaluate using the Length of the longest leaf

```{r LLL}

ggplot(completeBrass, aes(LLL,Flowers_Y_N))+
  geom_jitter(height = 0.25)

```


### Remove the outlier (probably a mistake in data entry)

#### Nota que se hace un subgrupo (subset) de los datos, usando la función subset(el data frame, la condición)


```{r LLL90}

ggplot(subset(completeBrass,LLL<90), aes(LLL,Flowers_Y_N, colour=BQS))+
  geom_jitter(height = 0.25)+
  binomial_smooth()
```


Choosing from different models, which is the best model.  

The R^2 versus the Akaike Information criterion (AIC)

https://www.theanalysisfactor.com/r-glm-model-fit/




Deviance

We see the word Deviance twice over in the model output. Deviance is a measure of goodness of fit of a generalized linear model. Or rather, it’s a measure of badness of fit–higher numbers indicate worse fit.

R reports two forms of deviance – the null deviance and the residual deviance. The null deviance shows how well the response variable is predicted by a model that includes only the intercept (grand mean).

For our example, we have a value of 910.24  on 1513 degrees of freedom. Including the independent variables (weight and displacement) decreased the deviance to 777.02 points on 1512 degrees of freedom, a significant reduction in deviance.

The Residual Deviance has reduced by 910.24-777.02=133.22 with a loss of one degrees of freedom.

```{r}

BrassModel.1 <- glm(Flowers_Y_N ~ Leaf_Num,
                    data = Brass, family = binomial())
summary(BrassModel.1)
```




Akaike Information Criteron

Information Criteria

The Akaike Information Criterion (AIC) provides a method for assessing the quality of your model through comparison of related models.  It’s based on the Deviance, but penalizes you for making the model more complicated.  Much like adjusted R-squared, it’s intent is to prevent you from including irrelevant predictors.

However, unlike adjusted R-squared, the number itself is not meaningful. If you have more than one similar candidate models (where all of the variables of the simpler model occur in the more complex models), then you should select the model that has the smallest AIC.

So it’s useful for comparing models, but isn’t interpretable on its own. YOU NEED TO COMPARE MODELS. 






```{r}


# Complex models

Brass$LLLxLN=Brass$LLL*Brass$Leaf_Num  # add a new variable the multiplication of LLL* Leaf_Num
#head(Brass)

BrassModel.1 <- glm(Flowers_Y_N ~ LLL,
                    data = Brass, family = binomial())


BrassModel.2 <- glm(Flowers_Y_N ~ Leaf_Num,
                    data = Brass, family = binomial())

BrassModel.3 <- glm(Flowers_Y_N ~ LLLxLN,
                    data = Brass, family = binomial())

summary(BrassModel.1)
summary(BrassModel.2)
summary(BrassModel.3)